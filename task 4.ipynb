{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14321e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'which' 不是內部或外部命令、可執行的程式或批次檔。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'which' 不是內部或外部命令、可執行的程式或批次檔。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 22.2.2 from C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip (python 3.9)\n",
      "\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai\n",
      "  Downloading openai-0.27.7-py3-none-any.whl (71 kB)\n",
      "     -------------------------------------- 72.0/72.0 kB 791.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (2.28.1)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp39-cp39-win_amd64.whl (323 kB)\n",
      "     -------------------------------------- 323.6/323.6 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->openai) (21.4.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-win_amd64.whl (34 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp39-cp39-win_amd64.whl (61 kB)\n",
      "     ---------------------------------------- 61.7/61.7 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->openai) (0.4.5)\n",
      "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.7 yarl-1.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script openai.exe is installed in 'C:\\Users\\maypa\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!python --version\n",
    "!which pip\n",
    "!pip --version\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb832255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\maypa\\appdata\\roaming\\python\\python39\\site-packages (0.27.7)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\maypa\\appdata\\roaming\\python\\python39\\site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\maypa\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\maypa\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\maypa\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->openai) (21.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\maypa\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\maypa\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->openai) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cbeafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10e12f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "df0982f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('01.txt', 'r', encoding='utf-8') as fh:\n",
    "    tmp = fh.read()\n",
    "    itemlist = tmp.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b8b4363e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffArtificial intelligence could lead to the extinction of humanity',\n",
       " ' experts - including the heads of OpenAI and Google Deepmind - have warned.\\nDozens have supported a statement published on the webpage of the\\xa0Centre for AI Safety.\\n\"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\" it reads.\\nBut others say the fears are overblown.\\nADVERTISEMENT\\nSam Altman',\n",
       " ' chief executive of ChatGPT-maker OpenAI',\n",
       " ' Demis Hassabis',\n",
       " ' chief executive of Google DeepMind and Dario Amodei of Anthropic have all supported the statement.\\nThe Centre for AI Safety website suggests a number of possible disaster scenarios:\\n· AIs could be weaponised - for example',\n",
       " ' drug-discovery tools could be used to build chemical weapons\\n· AI-generated misinformation could destabilise society and \"undermine collective decision-making\"\\n· The power of AI could become increasingly concentrated in fewer and fewer hands',\n",
       " ' enabling \"regimes to enforce narrow values through pervasive surveillance and oppressive censorship\"\\n· Enfeeblement',\n",
       " ' where humans become dependent on AI \"similar to the scenario portrayed in the film Wall-E\"\\nDr Geoffrey Hinton',\n",
       " ' who issued an earlier warning about risks from super-intelligent AI',\n",
       " \" has also supported the Centre for AI Safety's call.\\nYoshua Bengio\",\n",
       " ' professor of computer science at the university of Montreal',\n",
       " ' also signed.\\nDr Hinton',\n",
       " ' Prof Bengio and NYU Professor Yann LeCun are often described as the \"godfathers of AI\" for their groundbreaking work in the field - for which they jointly won the 2018 Turing Award',\n",
       " ' which recognises outstanding contributions in computer science.\\nBut Prof LeCun',\n",
       " ' who also works at Meta',\n",
       " ' has said these apocalyptic warnings are overblown\\xa0tweeting\\xa0that \"the most common reaction by AI researchers to these prophecies of doom is face palming\".\\n\\'Fracturing reality\\'\\nMany other experts similarly believe that fears of AI wiping out humanity are unrealistic',\n",
       " ' and a distraction from issues such as bias in systems that are already a problem.\\nArvind Narayanan',\n",
       " ' a computer scientist at Princeton University',\n",
       " ' has previously told the BBC that sci-fi-like disaster scenarios are unrealistic: \"Current AI is nowhere near capable enough for these risks to materialise. As a result',\n",
       " ' it\\'s distracted attention away from the near-term harms of AI\".\\nOxford\\'s Institute for Ethics in AI senior research associate Elizabeth Renieris told BBC News she worried more about risks closer to the present.\\n\"Advancements in AI will magnify the scale of automated decision-making that is biased',\n",
       " ' discriminatory',\n",
       " ' exclusionary or otherwise unfair while also being inscrutable and incontestable',\n",
       " '\" she said. They would \"drive an exponential increase in the volume and spread of misinformation',\n",
       " ' thereby fracturing reality and eroding the public trust',\n",
       " ' and drive further inequality',\n",
       " ' particularly for those who remain on the wrong side of the digital divide\".\\nMany AI tools essentially \"free ride\" on the \"whole of human experience to date\"',\n",
       " ' Ms Renieris said. Many are trained on human-created content',\n",
       " ' text',\n",
       " ' art and music they can then imitate - and their creators \"have effectively transferred tremendous wealth and power from the public sphere to a small handful of private entities\".\\nBut Centre for AI Safety director Dan Hendrycks told BBC News future risks and present concerns \"shouldn\\'t be viewed antagonistically\".\\n\"Addressing some of the issues today can be useful for addressing many of the later risks tomorrow',\n",
       " '\" he said.\\nSuperintelligence efforts\\nMedia coverage of the supposed \"existential\" threat from AI has snowballed since March 2023 when experts',\n",
       " ' including Tesla boss Elon Musk',\n",
       " ' signed an open letter\\xa0urging a halt\\xa0to the development of the next generation of AI technology.\\nThat letter asked if we should \"develop non-human minds that might eventually outnumber',\n",
       " ' outsmart',\n",
       " ' obsolete and replace us\".\\nIn contrast',\n",
       " ' the new campaign has a very short statement',\n",
       " ' designed to \"open up discussion\".\\nThe statement compares the risk to that posed by nuclear war. In a blog post OpenAI recently suggested superintelligence might be regulated in a similar way to nuclear energy: \"We are likely to eventually need something like an IAEA [International Atomic Energy Agency] for superintelligence efforts\" the firm wrote.\\n\\'Be reassured\\'\\nBoth Sam Altman and Google chief executive Sundar Pichai are among technology leaders to have discussed AI regulation recently with the prime minister.\\nSpeaking to reporters about the latest warning over AI risk',\n",
       " ' Rishi Sunak stressed the benefits to the economy and society.\\n\"You\\'ve seen that recently it was helping paralysed people to walk',\n",
       " ' discovering new antibiotics',\n",
       " ' but we need to make sure this is done in a way that is safe and secure',\n",
       " '\" he said.\\n\"Now that\\'s why I met last week with CEOs of major AI companies to discuss what are the guardrails that we need to put in place',\n",
       " ' what\\'s the type of regulation that should be put in place to keep us safe.\\n\"People will be concerned by the reports that AI poses existential risks',\n",
       " ' like pandemics or nuclear wars.\\n\"I want them to be reassured that the government is looking very carefully at this.\"\\nHe had discussed the issue recently with other leaders',\n",
       " ' at the G7 summit of leading industrialised nations',\n",
       " ' Mr Sunak said',\n",
       " ' and would raise it again in the US soon.\\nThe G7 has recently created a working group on AI.\\n\\n']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "73dc6661",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('02.txt', 'r', encoding='utf-8') as fh:\n",
    "    tmp2 = fh.read()\n",
    "    itemlist2 = tmp2.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0a3f7751",
   "metadata": {},
   "outputs": [],
   "source": [
    "itemlist2 = str(itemlist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "17d30ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyfile = open(\"key.txt\", \"r\")\n",
    "key = keyfile.readline()\n",
    "openai.api_key = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "23a3c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [itemlist[0:3200], itemlist[3201:6400]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2d1258d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffArtificial intelligence could lead to the extinction of humanity',\n",
       " ' experts - including the heads of OpenAI and Google Deepmind - have warned.\\nDozens have supported a statement published on the webpage of the\\xa0Centre for AI Safety.\\n\"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\" it reads.\\nBut others say the fears are overblown.\\nADVERTISEMENT\\nSam Altman',\n",
       " ' chief executive of ChatGPT-maker OpenAI',\n",
       " ' Demis Hassabis',\n",
       " ' chief executive of Google DeepMind and Dario Amodei of Anthropic have all supported the statement.\\nThe Centre for AI Safety website suggests a number of possible disaster scenarios:\\n· AIs could be weaponised - for example',\n",
       " ' drug-discovery tools could be used to build chemical weapons\\n· AI-generated misinformation could destabilise society and \"undermine collective decision-making\"\\n· The power of AI could become increasingly concentrated in fewer and fewer hands',\n",
       " ' enabling \"regimes to enforce narrow values through pervasive surveillance and oppressive censorship\"\\n· Enfeeblement',\n",
       " ' where humans become dependent on AI \"similar to the scenario portrayed in the film Wall-E\"\\nDr Geoffrey Hinton',\n",
       " ' who issued an earlier warning about risks from super-intelligent AI',\n",
       " \" has also supported the Centre for AI Safety's call.\\nYoshua Bengio\",\n",
       " ' professor of computer science at the university of Montreal',\n",
       " ' also signed.\\nDr Hinton',\n",
       " ' Prof Bengio and NYU Professor Yann LeCun are often described as the \"godfathers of AI\" for their groundbreaking work in the field - for which they jointly won the 2018 Turing Award',\n",
       " ' which recognises outstanding contributions in computer science.\\nBut Prof LeCun',\n",
       " ' who also works at Meta',\n",
       " ' has said these apocalyptic warnings are overblown\\xa0tweeting\\xa0that \"the most common reaction by AI researchers to these prophecies of doom is face palming\".\\n\\'Fracturing reality\\'\\nMany other experts similarly believe that fears of AI wiping out humanity are unrealistic',\n",
       " ' and a distraction from issues such as bias in systems that are already a problem.\\nArvind Narayanan',\n",
       " ' a computer scientist at Princeton University',\n",
       " ' has previously told the BBC that sci-fi-like disaster scenarios are unrealistic: \"Current AI is nowhere near capable enough for these risks to materialise. As a result',\n",
       " ' it\\'s distracted attention away from the near-term harms of AI\".\\nOxford\\'s Institute for Ethics in AI senior research associate Elizabeth Renieris told BBC News she worried more about risks closer to the present.\\n\"Advancements in AI will magnify the scale of automated decision-making that is biased',\n",
       " ' discriminatory',\n",
       " ' exclusionary or otherwise unfair while also being inscrutable and incontestable',\n",
       " '\" she said. They would \"drive an exponential increase in the volume and spread of misinformation',\n",
       " ' thereby fracturing reality and eroding the public trust',\n",
       " ' and drive further inequality',\n",
       " ' particularly for those who remain on the wrong side of the digital divide\".\\nMany AI tools essentially \"free ride\" on the \"whole of human experience to date\"',\n",
       " ' Ms Renieris said. Many are trained on human-created content',\n",
       " ' text',\n",
       " ' art and music they can then imitate - and their creators \"have effectively transferred tremendous wealth and power from the public sphere to a small handful of private entities\".\\nBut Centre for AI Safety director Dan Hendrycks told BBC News future risks and present concerns \"shouldn\\'t be viewed antagonistically\".\\n\"Addressing some of the issues today can be useful for addressing many of the later risks tomorrow',\n",
       " '\" he said.\\nSuperintelligence efforts\\nMedia coverage of the supposed \"existential\" threat from AI has snowballed since March 2023 when experts',\n",
       " ' including Tesla boss Elon Musk',\n",
       " ' signed an open letter\\xa0urging a halt\\xa0to the development of the next generation of AI technology.\\nThat letter asked if we should \"develop non-human minds that might eventually outnumber',\n",
       " ' outsmart',\n",
       " ' obsolete and replace us\".\\nIn contrast',\n",
       " ' the new campaign has a very short statement',\n",
       " ' designed to \"open up discussion\".\\nThe statement compares the risk to that posed by nuclear war. In a blog post OpenAI recently suggested superintelligence might be regulated in a similar way to nuclear energy: \"We are likely to eventually need something like an IAEA [International Atomic Energy Agency] for superintelligence efforts\" the firm wrote.\\n\\'Be reassured\\'\\nBoth Sam Altman and Google chief executive Sundar Pichai are among technology leaders to have discussed AI regulation recently with the prime minister.\\nSpeaking to reporters about the latest warning over AI risk',\n",
       " ' Rishi Sunak stressed the benefits to the economy and society.\\n\"You\\'ve seen that recently it was helping paralysed people to walk',\n",
       " ' discovering new antibiotics',\n",
       " ' but we need to make sure this is done in a way that is safe and secure',\n",
       " '\" he said.\\n\"Now that\\'s why I met last week with CEOs of major AI companies to discuss what are the guardrails that we need to put in place',\n",
       " ' what\\'s the type of regulation that should be put in place to keep us safe.\\n\"People will be concerned by the reports that AI poses existential risks',\n",
       " ' like pandemics or nuclear wars.\\n\"I want them to be reassured that the government is looking very carefully at this.\"\\nHe had discussed the issue recently with other leaders',\n",
       " ' at the G7 summit of leading industrialised nations',\n",
       " ' Mr Sunak said',\n",
       " ' and would raise it again in the US soon.\\nThe G7 has recently created a working group on AI.\\n\\n']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8e790a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatgptfn(sub_list):\n",
    "    result = ''\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an assistant\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{sub_list} :give me a summary with no execution\"}\n",
    "        ]\n",
    "    )\n",
    "    for choice in response.choices:\n",
    "        result += choice.message.content\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a1c4d5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2):\n",
    "    data[i] = chatgptfn(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ff32ead4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article examines the varying perspectives on the potential risks of AI to human beings. While some experts believe it poses a threat, others suggest addressing current issues such as AI bias. It stresses the importance of addressing future risks as well as present concerns like inequality and discriminatory decision-making. The G7 is reportedly working to establish regulations and safeguards around AI to mitigate potential risks.'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4350758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = ''\n",
    "for i in range(0,2):\n",
    "    data1 = data1 + data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2d1a6096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article examines the varying perspectives on the potential risks of AI to human beings. While some experts believe it poses a threat, others suggest addressing current issues such as AI bias. It stresses the importance of addressing future risks as well as present concerns like inequality and discriminatory decision-making. The G7 is reportedly working to establish regulations and safeguards around AI to mitigate potential risks.As an AI assistant, I am designed to assist users with various language-related tasks such as answering questions, generating text, summarizing information, and translating languages. My aim is to provide helpful and accurate assistance using advanced algorithms and machine learning techniques to interpret and analyze input data.'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5775658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = ''\n",
    "for i in range(0,2):\n",
    "    data2 = data2 + data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cf65728c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article examines the varying perspectives on the potential risks of AI to human beings. While some experts believe it poses a threat, others suggest addressing current issues such as AI bias. It stresses the importance of addressing future risks as well as present concerns like inequality and discriminatory decision-making. The G7 is reportedly working to establish regulations and safeguards around AI to mitigate potential risks.As an AI assistant, I am designed to assist users with various language-related tasks such as answering questions, generating text, summarizing information, and translating languages. My aim is to provide helpful and accurate assistance using advanced algorithms and machine learning techniques to interpret and analyze input data.'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fff2fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [data1, data2, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4140dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ba1da3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建詞袋\n",
    "texts = [[word for word in document.lower().split()] for document in data]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "82e55752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練 LDA 模型\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, \n",
    "                                            num_topics=3, random_state=100, update_every=1, \n",
    "                                            chunksize=100, passes=10, alpha='auto', per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a271ab75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 4.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numexpr in c:\\programdata\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.8.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from pyLDAvis) (63.4.1)\n",
      "Collecting pandas>=2.0.0\n",
      "  Downloading pandas-2.0.2-cp39-cp39-win_amd64.whl (10.7 MB)\n",
      "     ---------------------------------------- 10.7/10.7 MB 6.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Collecting funcy\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.9.1)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     -------------------------------------- 298.0/298.0 kB 6.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: gensim in c:\\programdata\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.1.2)\n",
      "Collecting numpy>=1.24.2\n",
      "  Downloading numpy-1.24.3-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "     ---------------------------------------- 14.9/14.9 MB 6.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.0.2)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "     -------------------------------------- 341.8/341.8 kB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2022.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (2.0.1)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from numexpr->pyLDAvis) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->numexpr->pyLDAvis) (3.0.9)\n",
      "Installing collected packages: funcy, tzdata, numpy, joblib, pandas, pyLDAvis\n",
      "Successfully installed funcy-2.0 joblib-1.2.0 numpy-1.24.3 pandas-2.0.2 pyLDAvis-3.4.1 tzdata-2023.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\maypa\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.6.0 requires daal==2021.4.0, which is not installed.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.24.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "47ac9d8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\maypa\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 391, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n  File \"C:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\nModuleNotFoundError: No module named 'pandas.core.indexes.numeric'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14288\\1380282904.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyLDAvis\\gensim.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \"\"\"\n\u001b[0;32m    122\u001b[0m     \u001b[0mopts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_extract_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mvis_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[0mterm_frequency\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm_topic_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m     topic_info = _topic_info(topic_term_dists, topic_proportion,\n\u001b[0m\u001b[0;32m    433\u001b[0m                              \u001b[0mterm_frequency\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_topic_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m                              n_jobs, start_index)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36m_topic_info\u001b[1;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs, start_index)\u001b[0m\n\u001b[0;32m    271\u001b[0m         ])\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m     top_terms = pd.concat(Parallel(n_jobs=n_jobs)\n\u001b[0m\u001b[0;32m    274\u001b[0m                           (delayed(_find_relevance_chunks)(log_ttd, log_lift, R, ls)\n\u001b[0;32m    275\u001b[0m                           for ls in _job_chunks(lambda_seq, n_jobs)))\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 975\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    976\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    565\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    566\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 567\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    568\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    444\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m                 \u001b[1;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": [
    "# 輸出主題模型分析結果\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65ffeef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
