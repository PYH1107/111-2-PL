{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fd51a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import urllib.request as req\n",
    "import json\n",
    "import time\n",
    "from time import sleep\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "790e40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.bbc.com/news' \n",
    "request = req.Request(url, headers = {\n",
    "    \"User-Agent\":'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'\n",
    "})\n",
    "with req.urlopen(request) as response:\n",
    "    data = response.read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5dd3d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "titles = soup.find_all(\"h3\", class_=\"gs-c-promo-heading__title gel-pica-bold nw-o-link-split__text__title\")\n",
    "for title in titles:\n",
    "    text = title.get_text()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5251984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do trains in India go off the tracks?\n",
      "Modi vows punishments over deadly India rail crash\n",
      "'My mother was missing - then I got a picture of her body'\n",
      "Drone shows extent of India train crash destruction\n",
      "How did three trains collide in India?\n",
      "Eyewitnesses describe pain and agony of India train crash\n",
      "Girl, 2, killed and 22 injured after Russian strike\n",
      "How many Russians have left during war - and who are they?\n",
      "How the Pakistani military feel about Imran Khan\n",
      "Winning UK show would mean bigger house - Ghetto Kids\n",
      "Violence in Germany over left-wing vigilante's jail term\n",
      "Three Israeli soldiers killed near Egypt border\n",
      "British Vogue editor Edward Enninful steps down\n",
      "Utah schools ban Bible for 'vulgarity and violence'\n",
      "Biden says debt deal averted 'economic collapse'\n",
      "Mystery of spy deaths in Italian boat accident\n",
      "River Plate game abandoned after fan falls to death\n",
      "Mystery of spy deaths in Italian boat accident\n",
      "River Plate game abandoned after fan falls to death\n",
      "Why is a top British morning TV show in crisis?\n",
      "Ill Rybakina withdraws from French Open\n",
      "Do Russians really hate the West?\n",
      "BBC World News TV\n",
      "BBC World Service Radio\n",
      "Parents clash in Pride protest at US primary school\n",
      "A war crimes case and what's next... in 83 seconds\n",
      "Performance, protest and pigs: Photos of the week\n",
      "Can US spelling bee champs spell British words?\n",
      "Your pictures on the theme of 'bridges'\n",
      "Caesarean by phone light - giving birth in a warzone\n",
      "Can sci-fi films teach us anything about an AI threat?\n",
      "Solar panels - an eco-disaster waiting to happen?\n",
      "Tracking the rise of Russia’s missile strikes on Kyiv\n",
      "Putin - South Africa's big headache\n",
      "Anger in Belgium over verdict in student's hazing death\n",
      "Beijing's comedy crackdown is hitting its music scene\n",
      "How old is too old to be a dad?\n",
      "The generation clocking the most hours\n",
      "A compelling new theory of Stonehenge\n",
      "8 ways to make your clothes last longer\n",
      "The birds ensnared by plastic\n",
      "Why Gen Z are so pay motivated\n",
      "The US' 113-mile 'floating' highway\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# 發送 GET 請求並獲取網頁內容\n",
    "url = \"https://www.bbc.com/news\"\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# 使用 BeautifulSoup 解析 HTML\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 找到所有標題的 <h3> 元素\n",
    "titles = soup.find_all(\"h3\", class_=\"gs-c-promo-heading__title\")\n",
    "\n",
    "# 提取標題文本並進行正規劃\n",
    "title_list = []\n",
    "for title in titles:\n",
    "    text = title.get_text(strip=True)  # 移除文本中的空白字符\n",
    "    text = re.sub(r\"\\s+\", \" \", text)   # 將連續的空白字符替換為單一空格\n",
    "    title_list.append(text)\n",
    "\n",
    "# 印出提取的標題\n",
    "for title in title_list:\n",
    "    print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f32ae022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do trains in India go off the tracks?\n",
      "Modi vows punishments over deadly India rail crash\n",
      "'My mother was missing - then I got a picture of her body'\n",
      "Drone shows extent of India train crash destruction\n",
      "How did three trains collide in India?\n",
      "Eyewitnesses describe pain and agony of India train crash\n",
      "Signal failure likely cause of rail crash - India minister\n",
      "Girl, 2, killed and 22 injured after Russian strike\n",
      "How the Pakistani military feel about Imran Khan\n",
      "How many Russians have left during war - and who are they?\n",
      "Winning UK show would mean bigger house - Ghetto Kids\n",
      "Violence in Germany over left-wing vigilante's jail term\n",
      "Three Israeli soldiers killed near Egypt border\n",
      "British Vogue editor Edward Enninful steps down\n",
      "Utah schools ban Bible for 'vulgarity and violence'\n",
      "Biden says debt deal averted 'economic collapse'\n",
      "Mystery of spy deaths in Italian boat accident\n",
      "Biden says debt deal averted 'economic collapse'\n",
      "Mystery of spy deaths in Italian boat accident\n",
      "River Plate game abandoned after fan falls to death\n",
      "Why is a top British morning TV show in crisis?\n",
      "Ill Rybakina withdraws from French Open\n",
      "Do Russians really hate the West?\n",
      "Parents clash in Pride protest at US primary school\n",
      "A war crimes case and what's next... in 83 seconds\n",
      "Performance, protest and pigs: Photos of the week\n",
      "Can US spelling bee champs spell British words?\n",
      "Your pictures on the theme of 'bridges'\n",
      "Caesarean by phone light - giving birth in a warzone\n",
      "Can sci-fi films teach us anything about an AI threat?\n",
      "Solar panels - an eco-disaster waiting to happen?\n",
      "Tracking the rise of Russia’s missile strikes on Kyiv\n",
      "Putin - South Africa's big headache\n",
      "Anger in Belgium over verdict in student's hazing death\n",
      "Beijing's comedy crackdown is hitting its music scene\n",
      "How old is too old to be a dad?\n",
      "The generation clocking the most hours\n",
      "A compelling new theory of Stonehenge\n",
      "8 ways to make your clothes last longer\n",
      "The birds ensnared by plastic\n",
      "Why Gen Z are so pay motivated\n",
      "The US' 113-mile 'floating' highway\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "\n",
    "# 發送 GET 請求並獲取網頁內容\n",
    "url = \"https://www.bbc.com/news\"\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# 使用 BeautifulSoup 解析 HTML\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 找到所有標題的 <h3> 元素\n",
    "titles = soup.find_all(\"h3\", class_=\"gs-c-promo-heading__title\")\n",
    "\n",
    "# 提取標題文本並進行正規劃和刪除指定內容\n",
    "title_list = []\n",
    "for title in titles:\n",
    "    text = title.get_text(strip=True)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"BBC World News TV\", \"\", text)\n",
    "    text = re.sub(r\"BBC World Service Radio\", \"\", text)\n",
    "    text = text.strip()  # 移除文本前後的空白字符\n",
    "    if text:\n",
    "        title_list.append(text)\n",
    "\n",
    "# 將結果存儲為 JSON 檔案\n",
    "with open(\"titles.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(title_list, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 印出提取的標題\n",
    "for title in title_list:\n",
    "    print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8ce2a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "     -------------------------------------- 636.8/636.8 kB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\maypa\\appdata\\roaming\\python\\python39\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.5)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27fdedf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "標題:  Why do trains in India go off the tracks?\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n",
      "標題:  Modi vows punishments over deadly India rail crash\n",
      "情緒分析:  Negative\n",
      "情緒極性:  -0.2\n",
      "\n",
      "標題:  'My mother was missing - then I got a picture of her body'\n",
      "情緒分析:  Negative\n",
      "情緒極性:  -0.2\n",
      "\n",
      "標題:  Drone shows extent of India train crash destruction\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n",
      "標題:  How did three trains collide in India?\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n",
      "標題:  Eyewitnesses describe pain and agony of India train crash\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n",
      "標題:  Signal failure likely cause of rail crash - India minister\n",
      "情緒分析:  Negative\n",
      "情緒極性:  -0.15833333333333335\n",
      "\n",
      "標題:  Girl, 2, killed and 22 injured after Russian strike\n",
      "情緒分析:  Negative\n",
      "情緒極性:  -0.1\n",
      "\n",
      "標題:  How the Pakistani military feel about Imran Khan\n",
      "情緒分析:  Negative\n",
      "情緒極性:  -0.1\n",
      "\n",
      "標題:  How many Russians have left during war - and who are they?\n",
      "情緒分析:  Positive\n",
      "情緒極性:  0.25\n",
      "\n",
      "標題:  Winning UK show would mean bigger house - Ghetto Kids\n",
      "情緒分析:  Positive\n",
      "情緒極性:  0.0625\n",
      "\n",
      "標題:  Violence in Germany over left-wing vigilante's jail term\n",
      "情緒分析:  Negative\n",
      "情緒極性:  -0.1\n",
      "\n",
      "標題:  Three Israeli soldiers killed near Egypt border\n",
      "情緒分析:  Negative\n",
      "情緒極性:  -0.05\n",
      "\n",
      "標題:  British Vogue editor Edward Enninful steps down\n",
      "情緒分析:  Negative\n",
      "情緒極性:  -0.07777777777777779\n",
      "\n",
      "標題:  Utah schools ban Bible for 'vulgarity and violence'\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n",
      "標題:  Biden says debt deal averted 'economic collapse'\n",
      "情緒分析:  Positive\n",
      "情緒極性:  0.2\n",
      "\n",
      "標題:  Mystery of spy deaths in Italian boat accident\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n",
      "標題:  Biden says debt deal averted 'economic collapse'\n",
      "情緒分析:  Positive\n",
      "情緒極性:  0.2\n",
      "\n",
      "標題:  Mystery of spy deaths in Italian boat accident\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n",
      "標題:  River Plate game abandoned after fan falls to death\n",
      "情緒分析:  Negative\n",
      "情緒極性:  -0.4\n",
      "\n",
      "標題:  Why is a top British morning TV show in crisis?\n",
      "情緒分析:  Positive\n",
      "情緒極性:  0.25\n",
      "\n",
      "標題:  Ill Rybakina withdraws from French Open\n",
      "情緒分析:  Negative\n",
      "情緒極性:  -0.16666666666666666\n",
      "\n",
      "標題:  Do Russians really hate the West?\n",
      "情緒分析:  Negative\n",
      "情緒極性:  -0.8\n",
      "\n",
      "標題:  Parents clash in Pride protest at US primary school\n",
      "情緒分析:  Positive\n",
      "情緒極性:  0.4\n",
      "\n",
      "標題:  A war crimes case and what's next... in 83 seconds\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n",
      "標題:  Performance, protest and pigs: Photos of the week\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n",
      "標題:  Can US spelling bee champs spell British words?\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n",
      "標題:  Your pictures on the theme of 'bridges'\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n",
      "標題:  Caesarean by phone light - giving birth in a warzone\n",
      "情緒分析:  Positive\n",
      "情緒極性:  0.4\n",
      "\n",
      "標題:  Can sci-fi films teach us anything about an AI threat?\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n",
      "標題:  Solar panels - an eco-disaster waiting to happen?\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n",
      "標題:  Tracking the rise of Russia’s missile strikes on Kyiv\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n",
      "標題:  Putin - South Africa's big headache\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n",
      "標題:  Anger in Belgium over verdict in student's hazing death\n",
      "情緒分析:  Negative\n",
      "情緒極性:  -0.7\n",
      "\n",
      "標題:  Beijing's comedy crackdown is hitting its music scene\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n",
      "標題:  How old is too old to be a dad?\n",
      "情緒分析:  Positive\n",
      "情緒極性:  0.1\n",
      "\n",
      "標題:  The generation clocking the most hours\n",
      "情緒分析:  Positive\n",
      "情緒極性:  0.5\n",
      "\n",
      "標題:  A compelling new theory of Stonehenge\n",
      "情緒分析:  Positive\n",
      "情緒極性:  0.21818181818181817\n",
      "\n",
      "標題:  8 ways to make your clothes last longer\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n",
      "標題:  The birds ensnared by plastic\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n",
      "標題:  Why Gen Z are so pay motivated\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n",
      "標題:  The US' 113-mile 'floating' highway\n",
      "情緒分析:  Neutral\n",
      "情緒極性:  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from textblob import TextBlob\n",
    "\n",
    "# 讀取JSON檔案\n",
    "with open(\"titles.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 使用TextBlob進行文本分析\n",
    "for title in data:\n",
    "    blob = TextBlob(title)\n",
    "\n",
    "    # 進行情緒分析\n",
    "    sentiment = blob.sentiment.polarity\n",
    "    if sentiment > 0:\n",
    "        sentiment_label = \"Positive\"\n",
    "    elif sentiment < 0:\n",
    "        sentiment_label = \"Negative\"\n",
    "    else:\n",
    "        sentiment_label = \"Neutral\"\n",
    "\n",
    "    # 顯示分析結果\n",
    "    print(\"標題: \", title)\n",
    "    print(\"情緒分析: \", sentiment_label)\n",
    "    print(\"情緒極性: \", sentiment)\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "041e85d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\maypa/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\maypa\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9100\\443670580.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# 分詞\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# 去除停用詞\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m     return [\n\u001b[0;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    877\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\maypa/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\maypa\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 設定日期和新聞頁面的URL\n",
    "date = \"2022-10-10\"\n",
    "url = f\"https://www.bbc.com/news/archive/{date}\"\n",
    "\n",
    "# 發送請求並獲取網頁內容\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# 使用BeautifulSoup解析HTML\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 獲取新聞標題文本\n",
    "titles = soup.find_all(\"h3\", class_=\"gs-c-promo-heading__title\")\n",
    "news_text = [title.get_text() for title in titles]\n",
    "\n",
    "# 合併所有標題文本\n",
    "all_text = \" \".join(news_text)\n",
    "\n",
    "# 文本預處理\n",
    "# 去除標點符號\n",
    "text = \"\".join([char for char in all_text if char.isalnum() or char.isspace()])\n",
    "# 轉為小寫\n",
    "text = text.lower()\n",
    "\n",
    "# 分詞\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# 去除停用詞\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# 計算頻率\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# 找到頻率最高的字詞\n",
    "most_common = word_counts.most_common(10)  # 取前10個最常見的字詞\n",
    "\n",
    "# 輸出結果\n",
    "for word, count in most_common:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63e50f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append(\"/path/to/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5ed7566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append(\"/path/to/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab2dbeee",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\maypa/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\maypa\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - '/path/to/nltk_data'\n    - '/path/to/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9100\\443670580.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# 分詞\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# 去除停用詞\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m     return [\n\u001b[0;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    877\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\maypa/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\maypa\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - '/path/to/nltk_data'\n    - '/path/to/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 設定日期和新聞頁面的URL\n",
    "date = \"2022-10-10\"\n",
    "url = f\"https://www.bbc.com/news/archive/{date}\"\n",
    "\n",
    "# 發送請求並獲取網頁內容\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# 使用BeautifulSoup解析HTML\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 獲取新聞標題文本\n",
    "titles = soup.find_all(\"h3\", class_=\"gs-c-promo-heading__title\")\n",
    "news_text = [title.get_text() for title in titles]\n",
    "\n",
    "# 合併所有標題文本\n",
    "all_text = \" \".join(news_text)\n",
    "\n",
    "# 文本預處理\n",
    "# 去除標點符號\n",
    "text = \"\".join([char for char in all_text if char.isalnum() or char.isspace()])\n",
    "# 轉為小寫\n",
    "text = text.lower()\n",
    "\n",
    "# 分詞\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# 去除停用詞\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# 計算頻率\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# 找到頻率最高的字詞\n",
    "most_common = word_counts.most_common(10)  # 取前10個最常見的字詞\n",
    "\n",
    "# 輸出結果\n",
    "for word, count in most_common:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa2380f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\maypa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b595e786",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\maypa/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\maypa\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - '/path/to/nltk_data'\n    - '/path/to/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\maypa/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\maypa\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - '/path/to/nltk_data'\n    - '/path/to/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9100\\443670580.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# 去除停用詞\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\maypa/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\maypa\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - '/path/to/nltk_data'\n    - '/path/to/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 設定日期和新聞頁面的URL\n",
    "date = \"2022-10-10\"\n",
    "url = f\"https://www.bbc.com/news/archive/{date}\"\n",
    "\n",
    "# 發送請求並獲取網頁內容\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# 使用BeautifulSoup解析HTML\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 獲取新聞標題文本\n",
    "titles = soup.find_all(\"h3\", class_=\"gs-c-promo-heading__title\")\n",
    "news_text = [title.get_text() for title in titles]\n",
    "\n",
    "# 合併所有標題文本\n",
    "all_text = \" \".join(news_text)\n",
    "\n",
    "# 文本預處理\n",
    "# 去除標點符號\n",
    "text = \"\".join([char for char in all_text if char.isalnum() or char.isspace()])\n",
    "# 轉為小寫\n",
    "text = text.lower()\n",
    "\n",
    "# 分詞\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# 去除停用詞\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# 計算頻率\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# 找到頻率最高的字詞\n",
    "most_common = word_counts.most_common(10)  # 取前10個最常見的字詞\n",
    "\n",
    "# 輸出結果\n",
    "for word, count in most_common:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b216c4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maypa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87bb776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 設定日期和新聞頁面的URL\n",
    "date = \"2022-10-10\"\n",
    "url = f\"https://www.bbc.com/news/archive/{date}\"\n",
    "\n",
    "# 發送請求並獲取網頁內容\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# 使用BeautifulSoup解析HTML\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 獲取新聞標題文本\n",
    "titles = soup.find_all(\"h3\", class_=\"gs-c-promo-heading__title\")\n",
    "news_text = [title.get_text() for title in titles]\n",
    "\n",
    "# 合併所有標題文本\n",
    "all_text = \" \".join(news_text)\n",
    "\n",
    "# 文本預處理\n",
    "# 去除標點符號\n",
    "text = \"\".join([char for char in all_text if char.isalnum() or char.isspace()])\n",
    "# 轉為小寫\n",
    "text = text.lower()\n",
    "\n",
    "# 分詞\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# 去除停用詞\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# 計算頻率\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# 找到頻率最高的字詞\n",
    "most_common = word_counts.most_common(10)  # 取前10個最常見的字詞\n",
    "\n",
    "# 輸出結果\n",
    "for word, count in most_common:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b02d92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 設定日期和新聞頁面的URL\n",
    "date = \"2022-10-10\"\n",
    "url = f\"https://www.bbc.com/news/archive/{date}\"\n",
    "\n",
    "# 發送請求並獲取網頁內容\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# 使用BeautifulSoup解析HTML\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 獲取新聞標題文本\n",
    "titles = soup.find_all(\"h3\", class_=\"gs-c-promo-heading__title\")\n",
    "news_text = [title.get_text() for title in titles]\n",
    "\n",
    "# 合併所有標題文本\n",
    "all_text = \" \".join(news_text)\n",
    "\n",
    "# 文本預處理\n",
    "# 去除標點符號\n",
    "text = \"\".join([char for char in all_text if char.isalnum() or char.isspace()])\n",
    "# 轉為小寫\n",
    "text = text.lower()\n",
    "\n",
    "# 分詞\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# 去除停用詞\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# 計算頻率\n",
    "word_counts = Counter(words)\n",
    "\n",
    "# 找到頻率最高的字詞\n",
    "most_common = word_counts.most_common(10)  # 取前10個最常見的字詞\n",
    "\n",
    "# 輸出結果\n",
    "for word, count in most_common:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96d9bc44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [404]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 設定日期和新聞頁面的URL\n",
    "date = \"2022-10-10\"\n",
    "url = f\"https://www.bbc.com/news/archive/{date}\"\n",
    "\n",
    "# 發送請求並獲取網頁內容\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a208c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
